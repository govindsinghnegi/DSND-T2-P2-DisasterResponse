{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26028, 39)\n",
      "   id                                            message  \\\n",
      "0   2  Weather update - a cold front from Cuba that c...   \n",
      "1   7            Is the Hurricane over or is it not over   \n",
      "2   8                    Looking for someone but no name   \n",
      "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
      "4  12  says: west side of Haiti, rest of the country ...   \n",
      "\n",
      "                                            original   genre  related  \\\n",
      "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
      "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
      "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
      "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
      "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
      "\n",
      "   request  offer  aid_related  medical_help  medical_products      ...        \\\n",
      "0        0      0            0             0                 0      ...         \n",
      "1        0      0            1             0                 0      ...         \n",
      "2        0      0            0             0                 0      ...         \n",
      "3        1      0            1             0                 1      ...         \n",
      "4        0      0            0             0                 0      ...         \n",
      "\n",
      "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
      "0            0                     0                0       0      0     0   \n",
      "1            0                     0                1       0      1     0   \n",
      "2            0                     0                0       0      0     0   \n",
      "3            0                     0                0       0      0     0   \n",
      "4            0                     0                0       0      0     0   \n",
      "\n",
      "   earthquake  cold  other_weather  direct_report  \n",
      "0           0     0              0              0  \n",
      "1           0     0              0              0  \n",
      "2           0     0              0              0  \n",
      "3           0     0              0              0  \n",
      "4           0     0              0              0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('MESSAGES', engine)\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "X = df['message'].values\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # normalize case and remove punctuation\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # lemmatize andremove stop words\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3)\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related      0.856     0.926     0.890      6026\n",
      "               request      0.799     0.426     0.555      1351\n",
      "                 offer      0.000     0.000     0.000        35\n",
      "           aid_related      0.756     0.583     0.659      3318\n",
      "          medical_help      0.687     0.084     0.150       678\n",
      "      medical_products      0.746     0.133     0.226       375\n",
      "     search_and_rescue      0.545     0.027     0.052       219\n",
      "              security      0.000     0.000     0.000       144\n",
      "              military      0.773     0.069     0.126       248\n",
      "                 water      0.843     0.178     0.294       511\n",
      "                  food      0.857     0.456     0.595       895\n",
      "               shelter      0.803     0.284     0.420       718\n",
      "              clothing      0.810     0.155     0.260       110\n",
      "                 money      0.500     0.011     0.022       180\n",
      "        missing_people      1.000     0.011     0.022        91\n",
      "              refugees      0.714     0.131     0.222       267\n",
      "                 death      0.774     0.141     0.239       340\n",
      "             other_aid      0.475     0.054     0.097      1069\n",
      "infrastructure_related      0.286     0.007     0.015       534\n",
      "             transport      0.516     0.043     0.080       368\n",
      "             buildings      0.703     0.116     0.199       388\n",
      "           electricity      0.250     0.006     0.011       170\n",
      "                 tools      0.000     0.000     0.000        49\n",
      "             hospitals      0.000     0.000     0.000        89\n",
      "                 shops      0.000     0.000     0.000        40\n",
      "           aid_centers      0.000     0.000     0.000       114\n",
      "  other_infrastructure      0.250     0.003     0.006       348\n",
      "       weather_related      0.841     0.611     0.708      2220\n",
      "                floods      0.917     0.384     0.541       690\n",
      "                 storm      0.759     0.391     0.516       749\n",
      "                  fire      0.750     0.039     0.074        77\n",
      "            earthquake      0.865     0.737     0.796       730\n",
      "                  cold      0.688     0.134     0.224       164\n",
      "         other_weather      0.618     0.081     0.144       418\n",
      "         direct_report      0.734     0.337     0.461      1530\n",
      "\n",
      "           avg / total      0.745     0.482     0.540     25253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=df.columns[4:], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7fc964380620>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7fc964380620>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': 42,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    'clf__estimator__n_estimators': [10, 25, 50]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vect__max_df': [0.5, 0.75, 1.0], 'tfidf__use_idf': [True, False], 'clf__estimator__n_estimators': [10, 25, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related      0.845     0.949     0.894      6026\n",
      "               request      0.847     0.512     0.638      1351\n",
      "                 offer      0.000     0.000     0.000        35\n",
      "           aid_related      0.767     0.669     0.715      3318\n",
      "          medical_help      0.786     0.081     0.147       678\n",
      "      medical_products      0.846     0.117     0.206       375\n",
      "     search_and_rescue      0.421     0.037     0.067       219\n",
      "              security      0.000     0.000     0.000       144\n",
      "              military      0.812     0.052     0.098       248\n",
      "                 water      0.865     0.352     0.501       511\n",
      "                  food      0.858     0.541     0.663       895\n",
      "               shelter      0.853     0.364     0.510       718\n",
      "              clothing      0.895     0.155     0.264       110\n",
      "                 money      0.714     0.028     0.053       180\n",
      "        missing_people      1.000     0.011     0.022        91\n",
      "              refugees      0.769     0.075     0.137       267\n",
      "                 death      0.872     0.121     0.212       340\n",
      "             other_aid      0.551     0.036     0.067      1069\n",
      "infrastructure_related      0.286     0.004     0.007       534\n",
      "             transport      0.744     0.087     0.156       368\n",
      "             buildings      0.775     0.142     0.240       388\n",
      "           electricity      0.667     0.012     0.023       170\n",
      "                 tools      0.000     0.000     0.000        49\n",
      "             hospitals      0.000     0.000     0.000        89\n",
      "                 shops      0.000     0.000     0.000        40\n",
      "           aid_centers      0.000     0.000     0.000       114\n",
      "  other_infrastructure      0.400     0.006     0.011       348\n",
      "       weather_related      0.848     0.691     0.761      2220\n",
      "                floods      0.917     0.464     0.616       690\n",
      "                 storm      0.785     0.491     0.604       749\n",
      "                  fire      0.000     0.000     0.000        77\n",
      "            earthquake      0.870     0.814     0.841       730\n",
      "                  cold      0.867     0.079     0.145       164\n",
      "         other_weather      0.647     0.026     0.051       418\n",
      "         direct_report      0.777     0.368     0.499      1530\n",
      "\n",
      "           avg / total      0.771     0.527     0.571     25253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=df.columns[4:], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier(random_state=42)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...timator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related      0.804     0.972     0.880      6026\n",
      "               request      0.764     0.523     0.621      1351\n",
      "                 offer      0.143     0.029     0.048        35\n",
      "           aid_related      0.769     0.612     0.681      3318\n",
      "          medical_help      0.625     0.240     0.347       678\n",
      "      medical_products      0.605     0.307     0.407       375\n",
      "     search_and_rescue      0.547     0.187     0.279       219\n",
      "              security      0.242     0.056     0.090       144\n",
      "              military      0.587     0.327     0.420       248\n",
      "                 water      0.725     0.603     0.658       511\n",
      "                  food      0.787     0.642     0.707       895\n",
      "               shelter      0.776     0.552     0.645       718\n",
      "              clothing      0.734     0.427     0.540       110\n",
      "                 money      0.495     0.256     0.337       180\n",
      "        missing_people      0.526     0.220     0.310        91\n",
      "              refugees      0.613     0.285     0.389       267\n",
      "                 death      0.757     0.459     0.571       340\n",
      "             other_aid      0.494     0.142     0.221      1069\n",
      "infrastructure_related      0.485     0.094     0.157       534\n",
      "             transport      0.636     0.204     0.309       368\n",
      "             buildings      0.619     0.415     0.497       388\n",
      "           electricity      0.616     0.265     0.370       170\n",
      "                 tools      0.222     0.041     0.069        49\n",
      "             hospitals      0.409     0.101     0.162        89\n",
      "                 shops      0.000     0.000     0.000        40\n",
      "           aid_centers      0.269     0.061     0.100       114\n",
      "  other_infrastructure      0.420     0.098     0.159       348\n",
      "       weather_related      0.848     0.680     0.755      2220\n",
      "                floods      0.881     0.559     0.684       690\n",
      "                 storm      0.749     0.526     0.618       749\n",
      "                  fire      0.467     0.273     0.344        77\n",
      "            earthquake      0.865     0.805     0.834       730\n",
      "                  cold      0.617     0.402     0.487       164\n",
      "         other_weather      0.434     0.141     0.213       418\n",
      "         direct_report      0.665     0.406     0.504      1530\n",
      "\n",
      "           avg / total      0.723     0.586     0.626     25253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=df.columns[4:], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7fc964380620>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7fc964380620>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': 42,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=42),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__max_df': [0.5, 0.75, 1.0],\n",
    "    'tfidf__use_idf': [True, False],\n",
    "    'clf__estimator__n_estimators': [10, 25, 50],\n",
    "    'clf__estimator__learning_rate': [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...timator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=42),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vect__max_df': [0.5, 0.75, 1.0], 'tfidf__use_idf': [True, False], 'clf__estimator__n_estimators': [10, 25, 50], 'clf__estimator__learning_rate': [0.1, 0.2, 0.5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related      0.795     0.982     0.879      6026\n",
      "               request      0.825     0.466     0.596      1351\n",
      "                 offer      0.000     0.000     0.000        35\n",
      "           aid_related      0.787     0.540     0.641      3318\n",
      "          medical_help      0.721     0.149     0.247       678\n",
      "      medical_products      0.720     0.227     0.345       375\n",
      "     search_and_rescue      0.634     0.119     0.200       219\n",
      "              security      0.667     0.028     0.053       144\n",
      "              military      0.706     0.194     0.304       248\n",
      "                 water      0.770     0.597     0.673       511\n",
      "                  food      0.808     0.699     0.750       895\n",
      "               shelter      0.820     0.483     0.608       718\n",
      "              clothing      0.844     0.345     0.490       110\n",
      "                 money      0.556     0.167     0.256       180\n",
      "        missing_people      0.706     0.132     0.222        91\n",
      "              refugees      0.662     0.199     0.305       267\n",
      "                 death      0.767     0.359     0.489       340\n",
      "             other_aid      0.586     0.064     0.115      1069\n",
      "infrastructure_related      0.579     0.041     0.077       534\n",
      "             transport      0.800     0.163     0.271       368\n",
      "             buildings      0.719     0.309     0.432       388\n",
      "           electricity      0.744     0.188     0.300       170\n",
      "                 tools      0.000     0.000     0.000        49\n",
      "             hospitals      0.625     0.056     0.103        89\n",
      "                 shops      0.000     0.000     0.000        40\n",
      "           aid_centers      0.429     0.026     0.050       114\n",
      "  other_infrastructure      0.412     0.020     0.038       348\n",
      "       weather_related      0.870     0.582     0.697      2220\n",
      "                floods      0.913     0.533     0.673       690\n",
      "                 storm      0.777     0.438     0.560       749\n",
      "                  fire      0.538     0.182     0.272        77\n",
      "            earthquake      0.870     0.805     0.836       730\n",
      "                  cold      0.712     0.317     0.439       164\n",
      "         other_weather      0.420     0.050     0.090       418\n",
      "         direct_report      0.747     0.313     0.441      1530\n",
      "\n",
      "           avg / total      0.761     0.538     0.587     25253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=df.columns[4:], digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score  is: 0.2438662934299358\n",
      "best parameters are: {'clf__estimator__learning_rate': 0.5, 'clf__estimator__n_estimators': 50, 'tfidf__use_idf': True, 'vect__max_df': 0.5}\n"
     ]
    }
   ],
   "source": [
    "print('best score  is: {}'.format(cv.best_score_))\n",
    "print('best parameters are: '.format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('disaster_response_model.pkl', 'wb') as pkl_file:\n",
    "    pickle.dump(cv, pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
